name: Fetch and Commit WHOISDS NRD Data

on:
  schedule:
    - cron: '0 17 * * *'  # 5:00 PM UTC = 4:00 AM AEST
  workflow_dispatch:

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
        with:
          persist-credentials: true
          fetch-depth: 0

      - name: Install dependencies
        run: sudo apt-get update && sudo apt-get install -y unzip curl

      - name: Download WHOISDS data (once for 60 days) + normalise daily domains
        run: |
          set -euo pipefail
          export LC_ALL=C

          mkdir -p whois_data
          TODAY=$(date -u +%Y-%m-%d)

          for i in $(seq 0 59); do
            DATE=$(date -u -d "$TODAY - $i days" +%Y-%m-%d)
            ZIP_NAME="$DATE.zip"
            ENCODED=$(echo -n "$ZIP_NAME" | base64)
            URL="https://www.whoisds.com/whois-database/newly-registered-domains/${ENCODED}/nrd"

            echo "Downloading $URL"
            curl -s -fL "$URL" -o "whois_data/$ZIP_NAME" || {
              echo "⚠️ Failed to download $ZIP_NAME"
              continue
            }

            if file "whois_data/$ZIP_NAME" | grep -q "Zip archive data"; then
              unzip -o "whois_data/$ZIP_NAME" -d whois_data/
              EXTRACTED=$(unzip -Z1 "whois_data/$ZIP_NAME" | grep -E '\.txt$' || true)
              if [ -n "$EXTRACTED" ]; then
                mv "whois_data/$EXTRACTED" "whois_data/$DATE.txt"

                # Create a normalised daily domains file (fast, single pass):
                # - trim whitespace
                # - drop blank lines + comment lines (# or ;)
                # - lower-case
                awk '
                  {
                    gsub(/^[ \t\r\n]+|[ \t\r\n]+$/, "", $0)
                    if ($0 == "" || $0 ~ /^[#;]/) next
                    print tolower($0)
                  }
                ' "whois_data/$DATE.txt" > "whois_data/$DATE.domains"
              fi
            else
              echo "⚠️ Invalid zip file: $ZIP_NAME (skipping)"
              rm -f "whois_data/$ZIP_NAME"
            fi
          done

      - name: Generate combined NRD files (txt + Pi-hole + AdGuard)
        run: |
          set -euo pipefail
          export LC_ALL=C

          TODAY=$(date -u +%Y-%m-%d)
          UPDATED_DATE=$(date -u +"%d %b %Y")

          mkdir -p pihole adguard

          for RANGE in 7 14 21 30 60; do
            OUT_TXT="nrd-${RANGE}.txt"
            TMP_DOMAINS="combined_${RANGE}.domains"

            echo "Combining files for last $RANGE days..."

            # Concatenate pre-normalised daily files, then dedupe once per range
            # (avoid rebuilding and re-normalising huge raw files)
            {
              i=0
              while [ "$i" -lt "$RANGE" ]; do
                DATE=$(date -u -d "$TODAY - $i days" +%Y-%m-%d)
                if [ -f "whois_data/$DATE.domains" ]; then
                  cat "whois_data/$DATE.domains"
                fi
                i=$((i+1))
              done
            } | sort -u > "$TMP_DOMAINS"

            DOMAIN_COUNT=$(wc -l < "$TMP_DOMAINS" | tr -d ' ')

            # 1) Original .txt output (headers + plain domains)
            {
              echo "# Title: NRD-${RANGE}days"
              echo "# Description: Newly registered domains over the past $RANGE days"
              echo "# @ GitHub : https://github.com/chrisjbawden/newly-registered-domains-tracker"
              echo "# Maintainer: Chris Bawden (chrisjbawden)"
              echo "# Updated: $UPDATED_DATE"
              echo "# Domain Count: $DOMAIN_COUNT"
              echo "#==============================================================="
              echo ""
              cat "$TMP_DOMAINS"
            } > "$OUT_TXT"

            # 2) Pi-hole regex list (apex + all subdomains)
            # Converts example.com -> (^|\.)example\.com$
            PIHOLE_OUT="pihole/nrd-${RANGE}.pihole"
            {
              echo "# Title: NRD-${RANGE}days (Pi-hole regex)"
              echo "# Description: Regex patterns to block newly registered domains and all subdomains over the past $RANGE days"
              echo "# Updated: $UPDATED_DATE"
              echo "# Domain Count: $DOMAIN_COUNT"
              echo ""
              awk '
                {
                  d=$0
                  # Escape regex metacharacters for the domain portion
                  gsub(/[][(){}.^$+*?|\\-]/, "\\\\&", d)
                  print "(^|\\\\.)" d "$"
                }
              ' "$TMP_DOMAINS"
            } > "$PIHOLE_OUT"

            # 3) AdGuard list (Adblock rules, apex + all subdomains)
            # Converts example.com -> ||example.com^
            ADGUARD_OUT="adguard/nrd-${RANGE}.adguard"
            {
              echo "! Title: NRD-${RANGE}days (AdGuard)"
              echo "! Description: Adblock rules to block newly registered domains and all subdomains over the past $RANGE days"
              echo "! Updated: $UPDATED_DATE"
              echo "! Domain Count: $DOMAIN_COUNT"
              echo ""
              awk '{ print "||" $0 "^" }' "$TMP_DOMAINS"
            } > "$ADGUARD_OUT"

            echo "Wrote: $OUT_TXT, $PIHOLE_OUT, $ADGUARD_OUT"
          done

      - name: Commit and push updated files
        run: |
          set -euo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add nrd-*.txt pihole/*.pihole adguard/*.adguard whois_data/*.domains

          git commit -m "Update WHOISDS NRD files [auto]" || echo "No changes to commit"
          git push
